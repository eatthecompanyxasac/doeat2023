{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>제품명</th>\n",
       "      <th>가격</th>\n",
       "      <th>평균평점</th>\n",
       "      <th>아이디</th>\n",
       "      <th>평점</th>\n",
       "      <th>날짜</th>\n",
       "      <th>종류</th>\n",
       "      <th>요약</th>\n",
       "      <th>리뷰</th>\n",
       "      <th>ko_review</th>\n",
       "      <th>token_nouns</th>\n",
       "      <th>token</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>허니바이허니 천연 벌꿀 스틱 90포입 국산 아카시아 5종 선물 세트 [원산지:국산]...</td>\n",
       "      <td>72,000</td>\n",
       "      <td>4.9</td>\n",
       "      <td>miri*******</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.08.16.</td>\n",
       "      <td>30포 선택1: 아카시아 / 30포 선택2: 아카시아 / 30포 선택3: 아카시아\\n</td>\n",
       "      <td>유통기한아주 넉넉해요포장꼼꼼해요편리편리해요</td>\n",
       "      <td>한달사용기재구매먹기 편하고 품질도 좋아요. 받는 손님도 좋아 하시고 구매등급도 따로...</td>\n",
       "      <td>한달사용기재구매먹기 편하고 품질도 좋아요 받는 손님도 좋아 하시고 구매등급도 따로 ...</td>\n",
       "      <td>['달', '사용', '기재', '구매', '품질', '손', '구매', '등급',...</td>\n",
       "      <td>[('한', 'XPN'), ('달', 'NNG'), ('사용', 'NNG'), ('...</td>\n",
       "      <td>['사용', '기재', '구매', '품질', '손', '구매', '등급', '문제'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>허니바이허니 천연 벌꿀 스틱 90포입 국산 아카시아 5종 선물 세트 [원산지:국산]...</td>\n",
       "      <td>72,000</td>\n",
       "      <td>4.9</td>\n",
       "      <td>dltm*****</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.07.28.</td>\n",
       "      <td>30포 선택1: 밤 / 30포 선택2: 밤 / 30포 선택3: 밤\\n</td>\n",
       "      <td>유통기한꽤 남았어요포장꼼꼼해요편리편리해요</td>\n",
       "      <td>간편하게 하루한포씩먹게되요~냉장이나 냉동으로 해서 시원하게 먹으니 맛잇어요!!밤꿀이...</td>\n",
       "      <td>간편하게 하루한포씩먹게되요냉장이나 냉동으로 해서 시원하게 먹으니 맛잇어요밤꿀이 많이...</td>\n",
       "      <td>['하루', '포', '냉장', '냉동', '밤꿀', '쓸줄알', '편', '생각'...</td>\n",
       "      <td>[('간편하', 'VA'), ('게', 'EC'), ('하루', 'NNG'), ('...</td>\n",
       "      <td>['하루', '냉장', '냉동', '밤꿀', '쓸줄알', '편', '생각', '요즘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>허니바이허니 천연 벌꿀 스틱 90포입 국산 아카시아 5종 선물 세트 [원산지:국산]...</td>\n",
       "      <td>72,000</td>\n",
       "      <td>4.9</td>\n",
       "      <td>yjdr****</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.08.06.</td>\n",
       "      <td>30포 선택1: 올인원 / 30포 선택2: 올인원 / 30포 선택3: 아카시아\\n</td>\n",
       "      <td>유통기한꽤 남았어요포장꼼꼼해요편리편리해요</td>\n",
       "      <td>홍삼이 여자몸에 안좋다고해서.. 그동안 안먹다가 꿀 스틱 보고 주문했습니다\\n당떨어...</td>\n",
       "      <td>홍삼이 여자몸에 안좋다고해서 그동안 안먹다가 꿀 스틱 보고 주문했습니다당떨어지기전에...</td>\n",
       "      <td>['홍삼', '여자', '몸', '동안', '꿀', '스틱', '당', '전', '...</td>\n",
       "      <td>[('홍삼', 'NNG'), ('이', 'JKS'), ('여자', 'NNG'), (...</td>\n",
       "      <td>['홍삼', '여자', '몸', '동안', '당', '밖', '지컷', '캡슐', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>허니바이허니 천연 벌꿀 스틱 90포입 국산 아카시아 5종 선물 세트 [원산지:국산]...</td>\n",
       "      <td>72,000</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2133***</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.07.13.</td>\n",
       "      <td>30포 선택1: 밤 / 30포 선택2: 야생화 / 30포 선택3: 올인원\\n</td>\n",
       "      <td>유통기한아주 넉넉해요포장꼼꼼해요편리편리해요</td>\n",
       "      <td>BEST꿀이 피로 회복, 기관지에 좋다고 하는데 사실 어느정도가 적당한지, 너무 많...</td>\n",
       "      <td>꿀이 피로 회복 기관지에 좋다고 하는데 사실 어느정도가 적당한지 너무 많이 먹으면 ...</td>\n",
       "      <td>['꿀', '피로', '회복', '기관지', '정도', '거', 'ㅠ', '부모',...</td>\n",
       "      <td>[('꿀', 'NNG'), ('이', 'JKS'), ('피로', 'NNG'), ('...</td>\n",
       "      <td>['피로', '회복', '기관지', '정도', '부모', '당', '수치', '걱정...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>허니바이허니 천연 벌꿀 스틱 90포입 국산 아카시아 5종 선물 세트 [원산지:국산]...</td>\n",
       "      <td>72,000</td>\n",
       "      <td>4.9</td>\n",
       "      <td>klys****</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.08.12.</td>\n",
       "      <td>30포 선택1: 야생화 / 30포 선택2: 밤 / 30포 선택3: 아카시아\\n</td>\n",
       "      <td>유통기한아주 넉넉해요포장꼼꼼해요편리편리해요</td>\n",
       "      <td>한달사용기진짜 꿀이예요. 예전 시골에서 먹던 꿀맛과 같아요. 술먹고 먹어도 숙취에도...</td>\n",
       "      <td>한달사용기진짜 꿀이예요 예전 시골에서 먹던 꿀맛과 같아요 술먹고 먹어도 숙취에도 좋...</td>\n",
       "      <td>['달', '사용기', '진짜', '꿀', '예전', '시골', '꿀맛', '술',...</td>\n",
       "      <td>[('한', 'MMN'), ('달', 'NNB'), ('사용기', 'NNG'), (...</td>\n",
       "      <td>['진짜', '예전', '시골', '꿀맛', '술', '숙취']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0             0           0   \n",
       "1             1             1           1   \n",
       "2             2             2           2   \n",
       "3             3             3           3   \n",
       "4             4             4           4   \n",
       "\n",
       "                                                 제품명      가격  평균평점  \\\n",
       "0  허니바이허니 천연 벌꿀 스틱 90포입 국산 아카시아 5종 선물 세트 [원산지:국산]...  72,000   4.9   \n",
       "1  허니바이허니 천연 벌꿀 스틱 90포입 국산 아카시아 5종 선물 세트 [원산지:국산]...  72,000   4.9   \n",
       "2  허니바이허니 천연 벌꿀 스틱 90포입 국산 아카시아 5종 선물 세트 [원산지:국산]...  72,000   4.9   \n",
       "3  허니바이허니 천연 벌꿀 스틱 90포입 국산 아카시아 5종 선물 세트 [원산지:국산]...  72,000   4.9   \n",
       "4  허니바이허니 천연 벌꿀 스틱 90포입 국산 아카시아 5종 선물 세트 [원산지:국산]...  72,000   4.9   \n",
       "\n",
       "           아이디   평점         날짜  \\\n",
       "0  miri*******  5.0  23.08.16.   \n",
       "1    dltm*****  5.0  23.07.28.   \n",
       "2     yjdr****  5.0  23.08.06.   \n",
       "3      2133***  5.0  23.07.13.   \n",
       "4     klys****  5.0  23.08.12.   \n",
       "\n",
       "                                                종류                       요약  \\\n",
       "0  30포 선택1: 아카시아 / 30포 선택2: 아카시아 / 30포 선택3: 아카시아\\n  유통기한아주 넉넉해요포장꼼꼼해요편리편리해요   \n",
       "1           30포 선택1: 밤 / 30포 선택2: 밤 / 30포 선택3: 밤\\n   유통기한꽤 남았어요포장꼼꼼해요편리편리해요   \n",
       "2    30포 선택1: 올인원 / 30포 선택2: 올인원 / 30포 선택3: 아카시아\\n   유통기한꽤 남았어요포장꼼꼼해요편리편리해요   \n",
       "3       30포 선택1: 밤 / 30포 선택2: 야생화 / 30포 선택3: 올인원\\n  유통기한아주 넉넉해요포장꼼꼼해요편리편리해요   \n",
       "4      30포 선택1: 야생화 / 30포 선택2: 밤 / 30포 선택3: 아카시아\\n  유통기한아주 넉넉해요포장꼼꼼해요편리편리해요   \n",
       "\n",
       "                                                  리뷰  \\\n",
       "0  한달사용기재구매먹기 편하고 품질도 좋아요. 받는 손님도 좋아 하시고 구매등급도 따로...   \n",
       "1  간편하게 하루한포씩먹게되요~냉장이나 냉동으로 해서 시원하게 먹으니 맛잇어요!!밤꿀이...   \n",
       "2  홍삼이 여자몸에 안좋다고해서.. 그동안 안먹다가 꿀 스틱 보고 주문했습니다\\n당떨어...   \n",
       "3  BEST꿀이 피로 회복, 기관지에 좋다고 하는데 사실 어느정도가 적당한지, 너무 많...   \n",
       "4  한달사용기진짜 꿀이예요. 예전 시골에서 먹던 꿀맛과 같아요. 술먹고 먹어도 숙취에도...   \n",
       "\n",
       "                                           ko_review  \\\n",
       "0  한달사용기재구매먹기 편하고 품질도 좋아요 받는 손님도 좋아 하시고 구매등급도 따로 ...   \n",
       "1  간편하게 하루한포씩먹게되요냉장이나 냉동으로 해서 시원하게 먹으니 맛잇어요밤꿀이 많이...   \n",
       "2  홍삼이 여자몸에 안좋다고해서 그동안 안먹다가 꿀 스틱 보고 주문했습니다당떨어지기전에...   \n",
       "3  꿀이 피로 회복 기관지에 좋다고 하는데 사실 어느정도가 적당한지 너무 많이 먹으면 ...   \n",
       "4  한달사용기진짜 꿀이예요 예전 시골에서 먹던 꿀맛과 같아요 술먹고 먹어도 숙취에도 좋...   \n",
       "\n",
       "                                         token_nouns  \\\n",
       "0  ['달', '사용', '기재', '구매', '품질', '손', '구매', '등급',...   \n",
       "1  ['하루', '포', '냉장', '냉동', '밤꿀', '쓸줄알', '편', '생각'...   \n",
       "2  ['홍삼', '여자', '몸', '동안', '꿀', '스틱', '당', '전', '...   \n",
       "3  ['꿀', '피로', '회복', '기관지', '정도', '거', 'ㅠ', '부모',...   \n",
       "4  ['달', '사용기', '진짜', '꿀', '예전', '시골', '꿀맛', '술',...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [('한', 'XPN'), ('달', 'NNG'), ('사용', 'NNG'), ('...   \n",
       "1  [('간편하', 'VA'), ('게', 'EC'), ('하루', 'NNG'), ('...   \n",
       "2  [('홍삼', 'NNG'), ('이', 'JKS'), ('여자', 'NNG'), (...   \n",
       "3  [('꿀', 'NNG'), ('이', 'JKS'), ('피로', 'NNG'), ('...   \n",
       "4  [('한', 'MMN'), ('달', 'NNB'), ('사용기', 'NNG'), (...   \n",
       "\n",
       "                                      cleaned_tokens  \n",
       "0  ['사용', '기재', '구매', '품질', '손', '구매', '등급', '문제'...  \n",
       "1  ['하루', '냉장', '냉동', '밤꿀', '쓸줄알', '편', '생각', '요즘...  \n",
       "2  ['홍삼', '여자', '몸', '동안', '당', '밖', '지컷', '캡슐', ...  \n",
       "3  ['피로', '회복', '기관지', '정도', '부모', '당', '수치', '걱정...  \n",
       "4                ['진짜', '예전', '시골', '꿀맛', '술', '숙취']  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\NT550\\asac\\SIH\\SIH\\code2\\result\\naver_shopping_preprocess.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 55763 entries, 0 to 57140\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0.2    55763 non-null  int64  \n",
      " 1   Unnamed: 0.1    55763 non-null  int64  \n",
      " 2   Unnamed: 0      55763 non-null  int64  \n",
      " 3   제품명             55763 non-null  object \n",
      " 4   가격              55763 non-null  object \n",
      " 5   평균평점            51729 non-null  float64\n",
      " 6   아이디             55763 non-null  object \n",
      " 7   평점              55763 non-null  float64\n",
      " 8   날짜              55763 non-null  object \n",
      " 9   종류              54410 non-null  object \n",
      " 10  요약              55763 non-null  object \n",
      " 11  리뷰              55763 non-null  object \n",
      " 12  ko_review       55763 non-null  object \n",
      " 13  token_nouns     55763 non-null  object \n",
      " 14  token           55763 non-null  object \n",
      " 15  cleaned_tokens  55763 non-null  object \n",
      "dtypes: float64(2), int64(3), object(11)\n",
      "memory usage: 7.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=['ko_review'], inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "# 불용어 리스트\n",
    "stop_words = ['선물','구매','제품','샘플','꿀','스틱','사용기','뭐','달','수','날','데','저','이번','ㅎ','포','ㅋ','ㅋㅋ','ㅎㅎ','ㅠ','ㅠㅠ','ㅜ','ㅜㅜ','달',\n",
    "              '거','것','꺼','어용','앞','저희','나','저','점','전','포','후','이', '있', '하', '것', '들', '그', '되', '수', '이',\n",
    "              '보','데', '듣','제','분','이거','번','손''않', '없', '나', '사람', '주', '아니', '등', '같', '우리', '때', '년', '가',\n",
    "              '한', '지', '대하', '오', '말', '일', '그렇', '위하','으','잘','시','ㅡ','만','개']\n",
    "\n",
    "# 불용어 제거 함수\n",
    "def remove_stop_words(tokens):\n",
    "    tokens_list = ast.literal_eval(tokens)  # 문자열 형태의 리스트를 실제 리스트로 변환\n",
    "    filtered_tokens = [token for token in tokens_list if token.strip() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# 'cleaned_tokens' 열에 있는 값에 불용어 제거 함수를 적용하여 새로운 열 생성\n",
    "naver_nouns['cleaned_tokens'] = naver_nouns['token_nouns'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "phrase input should be string, not <class 'float'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     processed_words \u001b[39m=\u001b[39m [word \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m다\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m morphemes]\n\u001b[0;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m processed_words\n\u001b[1;32m---> 15\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mfull_token\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39m리뷰\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(process_text)\n\u001b[0;32m     17\u001b[0m \u001b[39m# 모든 텍스트의 형태소를 하나의 리스트로 합치기\u001b[39;00m\n\u001b[0;32m     18\u001b[0m all_words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m words \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mfull_token\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m, in \u001b[0;36mprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_text\u001b[39m(text):\n\u001b[1;32m---> 11\u001b[0m     morphemes \u001b[39m=\u001b[39m okt\u001b[39m.\u001b[39;49mmorphs(text)\n\u001b[0;32m     12\u001b[0m     processed_words \u001b[39m=\u001b[39m [word \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m다\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m morphemes]\n\u001b[0;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m processed_words\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\konlpy\\tag\\_okt.py:89\u001b[0m, in \u001b[0;36mOkt.morphs\u001b[1;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmorphs\u001b[39m(\u001b[39mself\u001b[39m, phrase, norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, stem\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     87\u001b[0m     \u001b[39m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mreturn\u001b[39;00m [s \u001b[39mfor\u001b[39;00m s, t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos(phrase, norm\u001b[39m=\u001b[39;49mnorm, stem\u001b[39m=\u001b[39;49mstem)]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\konlpy\\tag\\_okt.py:69\u001b[0m, in \u001b[0;36mOkt.pos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos\u001b[39m(\u001b[39mself\u001b[39m, phrase, norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, stem\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, join\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m     \u001b[39m\"\"\"POS tagger.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m    In contrast to other classes in this subpackage,\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39m    this POS tagger doesn't have a `flatten` option,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39m    :param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     validate_phrase_inputs(phrase)\n\u001b[0;32m     71\u001b[0m     tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjki\u001b[39m.\u001b[39mtokenize(\n\u001b[0;32m     72\u001b[0m                 phrase,\n\u001b[0;32m     73\u001b[0m                 jpype\u001b[39m.\u001b[39mjava\u001b[39m.\u001b[39mlang\u001b[39m.\u001b[39mBoolean(norm),\n\u001b[0;32m     74\u001b[0m                 jpype\u001b[39m.\u001b[39mjava\u001b[39m.\u001b[39mlang\u001b[39m.\u001b[39mBoolean(stem))\u001b[39m.\u001b[39mtoArray()\n\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m join:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\konlpy\\tag\\_common.py:20\u001b[0m, in \u001b[0;36mvalidate_phrase_inputs\u001b[1;34m(phrase)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m\"\"\"validate if phrase input is provided in str format\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m    phrase (str): phrase input\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mphrase input should be string, not \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(phrase)\n\u001b[1;32m---> 20\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(phrase, basestring), msg\n",
      "\u001b[1;31mAssertionError\u001b[0m: phrase input should be string, not <class 'float'>"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Okt 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 형태소 분석 및 종결어미 추가하여 단어 리스트 생성\n",
    "def process_text(text):\n",
    "    morphemes = okt.morphs(text)\n",
    "    processed_words = [word + \"다\" for word in morphemes]\n",
    "    return processed_words\n",
    "\n",
    "df['full_token'] = df['리뷰'].apply(process_text)\n",
    "\n",
    "# 모든 텍스트의 형태소를 하나의 리스트로 합치기\n",
    "all_words = [word for words in df['full_token'] for word in words]\n",
    "\n",
    "# 단어 빈도 카운트\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# 워드클라우드 함수\n",
    "def wc_analysis(data):\n",
    "    words = dict(Counter(sum(data.to_list(), [])).most_common(300))  # 수정된 부분: word_counts -> Counter(sum(data.to_list(), []))\n",
    "    #img = Image.open(\"C:/Users/NT550-048/Desktop/잇더/rice.jpg\")\n",
    "    #imgArray = np.array(img)\n",
    "    wordcloud = WordCloud(font_path=r'C:\\Users\\NT550\\asac\\SIH\\SIH\\Kurly\\NanumBarunGothic.ttf', background_color='black', colormap=\"Accent_r\",\n",
    "                          width=800, height=500).generate_from_frequencies(words)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 정의 부분을 붙여넣으세요\n",
    "def wc_kw(kw_option, keyword):\n",
    "    if kw_option == 1:\n",
    "        filtered_df = df[df['리뷰'].apply(lambda x: isinstance(x, str) and keyword in x)]\n",
    "        if not filtered_df.empty:\n",
    "            return wc_analysis(filtered_df['full_token'])\n",
    "        else:\n",
    "            print('해당 키워드가 포함된 리뷰가 없습니다.')\n",
    "    else:\n",
    "        print('옵션 확인')\n",
    "\n",
    "def wc_analysis(data):\n",
    "    words = dict(Counter(sum(data.to_list(), [])).most_common(300))\n",
    "    wordcloud = WordCloud(font_path=r'C:\\Users\\NT550\\asac\\SIH\\SIH\\Kurly\\NanumBarunGothic.ttf', background_color='black', colormap=\"Accent_r\",\n",
    "                          width=800, height=500).generate_from_frequencies(words)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "anchor not supported for multiline text",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m wc_kw(\u001b[39m1\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m선물\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39mor\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39m패키지\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39mor\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39m포장\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m, in \u001b[0;36mwc_kw\u001b[1;34m(kw_option, keyword)\u001b[0m\n\u001b[0;32m      4\u001b[0m filtered_df \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39m리뷰\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39misinstance\u001b[39m(x, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m keyword \u001b[39min\u001b[39;00m x)]\n\u001b[0;32m      5\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filtered_df\u001b[39m.\u001b[39mempty:\n\u001b[1;32m----> 6\u001b[0m     \u001b[39mreturn\u001b[39;00m wc_analysis(filtered_df[\u001b[39m'\u001b[39;49m\u001b[39mfull_token\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      7\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m해당 키워드가 포함된 리뷰가 없습니다.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m, in \u001b[0;36mwc_analysis\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwc_analysis\u001b[39m(data):\n\u001b[0;32m     13\u001b[0m     words \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(Counter(\u001b[39msum\u001b[39m(data\u001b[39m.\u001b[39mto_list(), []))\u001b[39m.\u001b[39mmost_common(\u001b[39m300\u001b[39m))\n\u001b[0;32m     14\u001b[0m     wordcloud \u001b[39m=\u001b[39m WordCloud(font_path\u001b[39m=\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mNT550\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39masac\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mSIH\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mSIH\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mKurly\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mNanumBarunGothic.ttf\u001b[39;49m\u001b[39m'\u001b[39;49m, background_color\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mblack\u001b[39;49m\u001b[39m'\u001b[39;49m, colormap\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mAccent_r\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m---> 15\u001b[0m                           width\u001b[39m=\u001b[39;49m\u001b[39m800\u001b[39;49m, height\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)\u001b[39m.\u001b[39;49mgenerate_from_frequencies(words)\n\u001b[0;32m     16\u001b[0m     plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[0;32m     17\u001b[0m     plt\u001b[39m.\u001b[39mimshow(wordcloud)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\wordcloud\\wordcloud.py:508\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    505\u001b[0m transposed_font \u001b[39m=\u001b[39m ImageFont\u001b[39m.\u001b[39mTransposedFont(\n\u001b[0;32m    506\u001b[0m     font, orientation\u001b[39m=\u001b[39morientation)\n\u001b[0;32m    507\u001b[0m \u001b[39m# get size of resulting text\u001b[39;00m\n\u001b[1;32m--> 508\u001b[0m box_size \u001b[39m=\u001b[39m draw\u001b[39m.\u001b[39;49mtextbbox((\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m), word, font\u001b[39m=\u001b[39;49mtransposed_font, anchor\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    509\u001b[0m \u001b[39m# find possible places using integral image:\u001b[39;00m\n\u001b[0;32m    510\u001b[0m result \u001b[39m=\u001b[39m occupancy\u001b[39m.\u001b[39msample_position(box_size[\u001b[39m3\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmargin,\n\u001b[0;32m    511\u001b[0m                                    box_size[\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmargin,\n\u001b[0;32m    512\u001b[0m                                    random_state)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\PIL\\ImageDraw.py:727\u001b[0m, in \u001b[0;36mImageDraw.textbbox\u001b[1;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[0;32m    724\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m    726\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiline_check(text):\n\u001b[1;32m--> 727\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultiline_textbbox(\n\u001b[0;32m    728\u001b[0m         xy,\n\u001b[0;32m    729\u001b[0m         text,\n\u001b[0;32m    730\u001b[0m         font,\n\u001b[0;32m    731\u001b[0m         anchor,\n\u001b[0;32m    732\u001b[0m         spacing,\n\u001b[0;32m    733\u001b[0m         align,\n\u001b[0;32m    734\u001b[0m         direction,\n\u001b[0;32m    735\u001b[0m         features,\n\u001b[0;32m    736\u001b[0m         language,\n\u001b[0;32m    737\u001b[0m         stroke_width,\n\u001b[0;32m    738\u001b[0m         embedded_color,\n\u001b[0;32m    739\u001b[0m     )\n\u001b[0;32m    741\u001b[0m \u001b[39mif\u001b[39;00m font \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    742\u001b[0m     font \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetfont()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\PIL\\ImageDraw.py:774\u001b[0m, in \u001b[0;36mImageDraw.multiline_textbbox\u001b[1;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[39melif\u001b[39;00m anchor[\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtb\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    773\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39manchor not supported for multiline text\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m    776\u001b[0m widths \u001b[39m=\u001b[39m []\n\u001b[0;32m    777\u001b[0m max_width \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: anchor not supported for multiline text"
     ]
    }
   ],
   "source": [
    "wc_kw(1, '선물' or '패키지' or '포장')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
